from bs4 import BeautifulSoup
import requests
import re
URL = 'https://www.drom.ru/reviews/volvo/v40/5kopeek/'
# Например агригация данных с каких то других источников
# например множество фирм предоставляет услуги по сбору чистых данных в различных сферах(из печатных изданий, данные от ритейлров по продаже товаров и т,д)
# основа этих технологий - это парсинг html документов
# 1 Получить URL страницы с которой хотим извлечь данные
# 2 Скопировать или загрузить HTML содержимое страницы
# 3 Распарсить содержимое и получить необходимые данные
# СКРАПИНГ Scraping это парсинг вебстраниц/HTML ( ПАРСИНГ БЫВАЕТ ВООБЩЕ ЧЕГО УГОДНО документа ворд или базы данных и т.д)
# Будем парсить отзывы об автомобилязх Volvo c сайта drom.ru


print('--------------')
# Для начала сделаем обычный get запрос к этому url и получим это полотно циликом
page = requests.get(URL)
# нам потребуется код подтверждения - ответ от сервера что запрос был успешным
print(page.status_code)
print(page.text)
# НАЧИНАЕМ ИСПОЛЬЗОВАТЬ БИБЛИОТЕКУ BeautifulSoup - указываем что будем парсить  html
# создаем специальный объект
soup = BeautifulSoup(page.text, 'html.parser')
print(type(soup))        #  <class 'bs4.BeautifulSoup'>

# посмотри оглавление документа  - атрибут title
# ВООБЩЕ МОЖНО ОБРАЩАТЬСЯ К ЛЮБОМУ ТЕГУ - ЭТО ВЕРНЕТ ПЕРВЫЙ ТЕГ
print(soup.title)      # <title>Вольво V40 отзывы владельцев: все минусы и недостатки</title>
# извлекаем текст  - вернет строку со специальными возможностями
print(soup.title.string, type(soup.title.string))  # Вольво V40 отзывы владельцев: все минусы и недостатки <class 'bs4.element.NavigableString'
# что бы получить обычную строку- нужно указать атрибут text - в этом атрибуте будет лежать то что находится между тегами
print(soup.title.text, type(soup.title.text))  # Вольво V40 отзывы владельцев: все минусы и недостатки <class 'str'>
print('-----ПОЛУЧИМ ВСЕ ССЫЛКИ С ДАННОЙ html страницы')
print(soup.a)
print(soup.a.text)    # содержит то что между тегами

# что бы получить ссылку есть у объекта soup  специальный метод get который может брать любые атрибуты которые содеражтся в данном теге
print(soup.a.get('href'), type(soup.a.get('href')))  # обаращиние к атрибуту тега - так получили ссылку

# метод findall что бы получит все ссылки
# создаем спец-объект (множество) в нем будут храниться все ссылки
a_tags = soup.find_all('a')
print(type(a_tags))     # <class 'bs4.element.ResultSet'>
print(len(a_tags))
for ref in a_tags:
    print(ref.get('href')) # вывели все ссылки на экран
# теперь осталось отфильтровать по какому-то критерию - например по налачию http  в начале
# [\S]+  любой непробельный символ
print("---решим эту задачу с помощью регулярных выражений")
pattern = r'https?://[\S]+'
ref_all = re.findall(pattern, page.text)
print(len(ref_all))
print(ref_all)

# С помощью BeatifulSoup данные получаются чище -потому что мы берем из определенного места
# С помощью find_all можно находить любые объекты например найти все блоки div
div_tags = soup.find_all('div')
print(len(div_tags))